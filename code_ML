import os
import numpy as np
import requests    # permission  
                    #Fetching Web Pages: Scraping data from web pages by sending GET requests and parsing the HTML content.
                    #Interacting with APIs: Communicating with RESTful APIs by sending requests with parameters, headers, and payloads, and processing the JSON responses.
                    #Downloading Files: Downloading files from the web by sending GET requests and saving the response content to a file.
                    #Form Submission: Submitting web forms programmatically by sending POST requests with form data.

from bs4 import BeautifulSoup    #   webScrapping package # bs4 beautifulSoup 4th verson
# breaking down a given text or file into smaller parts that can be easily analyzed and manipulated.

url = 'https://www.imdb.com/chart/top/'
requests.get(url)

url1 = 'https://www.worldometers.info/coronavirus/#countries'
requests.get(url1)

url2 = 'https://www.ambitionbox.com/list-of-companies?page=1'
requests.get(url2)

url3 = 'https://www.learnbay.co/'
requests.get(url3)



# Setting a custom User-Agent header in your request can sometimes help bypass restrictions that result in a 403 Forbidden response. Here is how you can incorporate the custom User-Agent header in your request using the requests library in Python

headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; X64) Apple WeKit /537.36(KHTML, like Gecko) Chrome/80.0.3987.162 Safari/537.36'}

html = requests.get('https://www.imdb.com/chart/top/', headers=headers).text

soup = BeautifulSoup(html)
soup

soup.find('title').getText()

soup.findAll('h1')

soup.find_all('h2', {'class':'ipc-title__text'})

soup.findAll('h2', {'class':'ipc-title__text'})

titleList = soup.find_all('h3', {'class':'ipc-title__text'})

titleList

for title in titleList:
    print(title.getText())

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
%matplotlib inline
import seaborn as sns
sns.set()
import warnings
warnings.filterwarnings('ignore')
import sklearn, scipy
from datetime import date, datetime
from urllib.request import Request, urlopen
from bs4 import BeautifulSoup as soup
import plotly.graph_objects as go
import plotly.express as px
import plotly.offline as py
import gc

url = 'https://www.worldometers.info/coronavirus/#countries'
requests.get(url).text

"""
url = "https://www.ambitionbox.com/list-of-companies?page=1"
requests.get(url).text
"""

# above code is only message that it i surl

url = 'https://www.worldometers.info/coronavirus/#countries'
req = Request(url)
webpage = urlopen(req)   # this part contain all the url data
print(webpage)

# this only to check that the machine is working or not
# i.e the website is responding

page_soup = BeautifulSoup(webpage, 'html.parser') # "lxml"
#page_soup = BeautifulSoup(webpage, 'lxml')
page_soup

today = datetime.now()
print(today)

yestarday = "%s %d, %d" %(date.today().strftime('%b'),today.day-1, today.year)
print(yestarday)
print()
print(today)

date.today().strftime('%b')        #functions for current month

table = page_soup.findAll("table",{'id':'main_table_countries_yesterday'})
table
# it fetch the data from the html website where the table include id and countries for that id.

containers = table[0].findAll('tr',{"style":""}) # from the 1st table it fetch all the rows
title = containers[0]  # from the 1st table it fetch only 1st row
title

containers[1] # from 1st table it fetch the second row

del containers[0]

table = page_soup.findAll("table",{'id':'main_table_countries_yesterday'}) # it contain parse data with id/countries 
containers = table[0].findAll('tr',{"style":""}) #it contain all data of 1st table
title = containers[0] # 1st table - 1st row data transfer to title 
del containers[0] # deleted


all_data  = [] # empty list
clean = True # that is also clean

for country in containers:       # for loop in the containers work untill country(table) is there
    country_data = []            # empty list
    country_container = country.findAll('td')     #country(table) all data from each cell
    if country_container[1].text =='China':       # leaving the 1st column,selecting the 2nd column country one. 
        continue                                  # above loop continue till it matches chine
    for i in range(1, len(country_container)):    # for loop will till the end of text
        final_feature = country_container[i].text # at each step row data transfer to finial_feature 
        if clean:                                 
            if i!= 1 and i!=len(country_container)-1:           # this if loop will wirk till start to end
                final_feature = final_feature.replace(",","")   # remove comma from data
                if final_feature.find("+") != -1:               # find '+'
                    final_feature = final_feature.replace("+","") # replacing + 
                    final_feature = float(final_feature)          #
                #if final_feature.find("_") != -1:
                    #final_feature = float(final_feature)*-1
            if final_feature =='N/A':                    # include null value.
                final_feature = 0                        # find null value there make it zero. 
            elif final_feature =="" or final_feature == " ":    # for null, gap make it -1
                final_feature= -1
            country_data.append(final_feature)            # add one after the other, all the data
    all_data.append(country_data)                         # add data append in all_data.

all_data

df = pd.DataFrame(all_data)
df.head()

df.drop([15,16,17,18,19,20], axis=1, inplace=True)

df.head()

column_labels = ['Country', 'Total Cases','New Cases','Total Deaths','New Deaths','Total Recovered',
                 'New Recovered','Active Cases','Serious Critical','Total Cases/1M pop',
                 'Death/1M pop','Total Tests','Tests/1M pop','Population','Continent']

df.columns = column_labels
df

df.info()

for label in df.columns:      # for loop, continue till the last column 
    if label !='Country' and label !='Continent': # column should not be
        df[label] = pd.to_numeric(df[label])   #convert each cell of data to numerical value
        
        

df

df.info()

sns.relplot(x = 'Total Cases', y = 'Total Recovered', data=df, kind='line')
plt.show()

df["%inc Cases"] = df['New Cases']/df['Total Cases']*100
df["%inc Deaths"] = df['New Deaths']/df['Total Deaths']*100
df["%inc Recovered"] = df['New Recovered']/df['Total Recovered']*100

df.head()

cases = df[['Total Cases','Active Cases','Total Deaths']].loc[0]
cases_df = pd.DataFrame(cases).reset_index()
cases_df.columns = ['Type','Total']
cases_df['Percentage'] = np.round(100*cases_df['Total']/np.sum(cases_df['Total']),2)
cases_df['Virus'] = ["COVID-19" for i in range(len(cases_df))]
fig = px.bar(cases_df, x='Virus', y='Percentage', color='Type', hover_data=['Total'])
fig.show()

cases = df[['New Cases','New Deaths','New Recovered']].loc[0]  # extract three column & convert 1st column into rows
cases_df = pd.DataFrame(cases).reset_index()  # reset the index "desc order of numerical value"
cases_df.columns = ['Type','Total']   # 
cases_df['Percentage'] = np.round(100*cases_df['Total']/np.sum(cases_df['Total']),2)
cases_df['Virus'] = ["COVID-19" for i in range(len(cases_df))]
fig = px.bar(cases_df, x='Virus', y='Percentage', color='Type', hover_data=['Total'])
fig.show()

df1 = df.drop([len(df)-1])
country_df = df1.drop([0])
country_df

country_df.shape

Top_Countries = 5
country = country_df.columns[1:14]

fig = go.Figure()
c = 0

for i in country_df.index:
    if c <Top_Countries:
        fig.add_trace(go.Bar(name=country_df['Country'][i],
                            x=country, y = country_df.loc[i][1:14]))
    else:
        break
    c +=1
fig.update_layout(title={"text":f'top {Top_Countries} countries affected'}, yaxis_type="log")
fig.show()

df['Continent'].value_counts() 
#is used in pandas to count the occurrences of each unique value in the 'Continent' column of a DataFrame df
/*   Country      Continent
0      USA  North America
1   Canada  North America
2   Brazil  South America
3  Germany         Europe
4   France         Europe
5    China           Asia
6    India           Asia
output -
North America    2
Europe           2
Asia             2
South America    1
Name: Continent, dtype: int64

*/

continent_df = df.groupby('Continent').sum().drop('All')
continent_df = continent_df.reset_index()
continent_df
/*  
continent_df = df.groupby('Continent').sum()

Country      Continent  Cases  Deaths
0      USA  North America   1000      50
1   Canada  North America    500      30
2   Brazil  South America    700      20
3  Germany         Europe    800      40
4   France         Europe    600      30
5    China           Asia   1200     100
6    India           Asia   1500     120

continent_df = continent_df.reset_index()
In this example, there is no row labeled 'All', so this step has no effect.
                Cases  Deaths
Continent                     
Asia              2700     220
Europe            1400      70
North America     1500      80
South America      700      20


      Continent  Cases  Deaths
0           Asia   2700     220
1         Europe   1400      70
2  North America   1500      80
3  South America    700      20

*/

cases_list = ["Total Cases","Total Deaths","Total Recovered","New Cases","New Deaths"]

Function Definition:

def continent_visualization(v_list):
Defines a function named continent_visualization that takes a list v_list as an argument.

def continent_visaulization(v_list):
    for label in v_list:
        c_df = continent_df[['Continent', label]] #Selects the 'Continent' column and the current variable column from continent_df.
        c_df['Percentage'] = np.round(100*c_df[label]/np.sum(c_df[label]),2)
        c_df['Virus'] = ["COVID-19" for i in range(len(c_df))]
        fig = px.bar(c_df, x='Virus', y='Percentage', color='Continent', hover_data=[label])
        fig.update_layout(title={"text": f"{label}"})
                                 
        fig.show()
                                 
        gc.collect()

continent_visaulization(cases_list)

























